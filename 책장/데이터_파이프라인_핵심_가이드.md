# 데이터 파이프라인 핵심 가이드

## 데이터 파이프라인 소개

### 데이터 파이프라인이란?

다양한 소스에서 새로운 가치를 얻을 수 있는 대상으로 데이터를 옮기고 변환하는 일련의 과정이다.
분석, 리포팅, 머신러닝 능력의 기초가 된다.

데이터 파이프라인은 Rest API와 같은 단일 소스에서 데이터를 추출하고 데이터 웨어하우스의 SQL 테이블과 같은 다른 대상으로 데이터를 옮기는 간단한 구조의 파이프라인부터 추출된 데이터에 대한 유효성 검사, 가공 과정, 머신러닝 모델 추론과 같은 과정들이 추가되는 복잡한 구조의 파이프라인까지 다양한 복잡성을 가진다.

### 누가 파이프라인을 구축할까?

데이터 파이프라인은 누구라도 구축 가능하나 주로 데이터 엔지니어가 구축한다.

데이터 엔지니어의 목적은 단순히 데이터를 데이터 웨어하우스에 로드하는 것 뿐만 아니라 데이터를 통해 통찰력을 만들어내는 데이터 과학자, 데이터 분석가와 협업해 분석에 집중할 수 있도록 확장 가능한 프로덕션 상태로 전환하는 것이다.

데이터 엔지니어의 특정 기술은 조직에서 사용하는 기술 스택에 따라 달라지나 몇몇 공통적인 보유 기술이 있다.

1.   SQL과 데이터 웨어하우징 기초
     -   DB를 다루기 위해 쿼리하는 방법을 알아야 한다.
         SQL은 DB 쿼리에 보편적인 언어이다.
     -   단순 SQL 뿐만 아니라 SQL 튜닝법(고성능 SQL 작성법)과 데이터 모델링의 기본 사항을 알아야 한다.
2.   Python, Java
     -   주로 Python, Java를 사용한다.
     -   최근에는 Go 언어도 등장한다.
3.   분산 컴퓨팅
     -   데이터 양이 많아지며 분산 컴퓨팅 플랫폼이 사용되고 있다.
         분산 컴퓨팅은 여러 시스템의 성능을 결합해 대량의 데이터를 효율적으로 저장, 처리, 분석할 수 있다.
4.   기본 시스템 관리
     -   데이터 엔지니어는 리눅스 명령줄에 능숙해야 한다.
     -   응용 프로그램 로그 분석, 크론 작업 예약, 방화벽 및 기타 보안 설정과 같은 작업이 요구된다.
5.   목표 지향적 사고 방식
     -   데이터 엔지니어는 팀의 데이터 분석가, 데이터 과학자와 정기적으로 소통하고 협엽한다.
         파이프라인을 구축하는 이유를 알고 있으면 더 나은 아키텍처적인 결정을 내릴 수 있다.

### 왜 데이터 파이프라인을 구축할까?

데이터 과학자와 분석가는 데이터를 분석해 비즈니스적인 통찰력을 제공하는데 전문성이 있다.
원본 데이터를 수집하고 처리하는 데 시간을 낭비하지 않고 잘 정제된 데이터를 통해 분석 업무만을 수행해야 효율적이다.

데이터 과학자와 분석가가 데이터를 더 가치있고 효율적으로 사용할 수 있도록 하기 위해 데이터 파이프라인 구축이 필요하다.

## 최신 데이터 인프라

### 데이터 소스의 다양성

대부분의 조직에는 분석 작업의 수행 대상이 되는 수십 ~ 수백개의 데이터 소스가 존재한다.

데이터 소스들은 여러 차원으로 구분될 수 있다.

#### 소스 시스템 소유권

데이터 수집(Data Ingestion)이란 한 소스에서 데이터를 추출해 다른 소스로 옮기는 행위이다.

데이터 수집의 대상이 되는 소스 시스템은 위치에 따라 **타사 데이터 소스에 위치한 데이터**나 내부적으로 **구축된 시스템**으로 구별된다.

1.   타사 데이터 소스에 위치한 데이터 소스
     -   예시로 Rest API를 통한 데이터 접근은 제공하나 SQL DB에 직접 접근하지 못하게 하는 경우가 흔하다.
     -   통신 속도와 같은 제한이 생길 수 있고, 접근 가능한 데이터와 세부 수준까지 분석가가 필요로 하는 환경에 맞게 지정하기 어렵다.
2.   내부적으로 구축된 시스템
     -   접근 방법 뿐 아니라 데이터의 세부 수준을 분석가가 필요한 환경에 맞춰 지정할 수 있다.
     -   시스템을 구축하는 단계에서 발생할 수 있는 잠재적인 문제들을 고려해야 한다.
         -   데이터 수집이 시스템에 부하를 가하는가?
         -   데이터를 점진적으로 옮길 수 있는가?

#### 수집 인터페이스와 데이터 구조

소스 데이터의 소유권과 관계 없이 데이터 엔지니어는 소스 데이터를 얻는 방법과 형식을 우선 고려해야 한다.

##### 데이터 인터페이스

1.   PostgreSQL이나 MySQL DB와 같이 애플리케이션 뒤에 있는 DB
2.   Rest API와 같은 시스템 상단의 추상화 계층
3.   Apache Kafka와 같은 스트림 처리 플랫폼
4.   CSV 파일과 같이 기타 플랫 파일을 포함하는 공유 네트워크 파일 시스템(Network File System)나 클라우드 스토리지 버킷
5.   데이터 웨어하우스나 데이터 레이크
6.   HDFS이나 HBase 데이터베이스의 데이터

##### 데이터 구조

1.   Rest API의 JSON
2.   DB의 잘 구성된 데이터
3.   DB 테이블의 열 내의 JSON
4.   반정형화된 로그 데이터
5.   CSV, 고정 폭 형식(FWF)과 기타 플랫 파일 형식
6.   플랫 파일의 JSON
7.   Kafka의 스트림 출력

잘 구성된 데이터는 파이프라인 구축과 분석 작업을 진행하기 쉬울 수 있다.
하지만 해당 데이터는 일반적으로 애플리케이션이나 웹 사이트를 위해 정형화되어 있다.
분석 작업에 더 적합한 형태로 정형화하기 위해 데이터 수집 외에 클렌징, 변환 작업 등을 파이프라인에 구축해야 한다.

JSON과 같은 반정형 데이터의 경우 관계형 DB와 달리 같은 데이터 셋 안에 데이터의 무결성을 보장하기 어렵다.
같은 데이터 셋 내에 있더라도 누락되거나 구조가 다른 데이터가 포함될 수 있다(파이프라인에서 처리해줘야 한다.).

비정형 데이터는 자연어 처리(NLP)나 컴퓨터 비전(CV)과 같이 이미지, 비디오, 오디오, 텍스트 등의 데이터를 의미한다.
파이프라인 구성 시에 데이터 형식을 고려해줘야 한다.

#### 데이터 사이즈

데이터 엔지니어는 파이프라인의 각 단계를 설정할 때 데이터 사이즈를 고려한다.
데이터 사이즈가 크다고 가치가 비례하게 증가하지는 않기에 대용량 / 소용량 두 가지로 데이터 셋을 구별하기 보단 스펙트럼의 측면에서 데이터 사이즈를 고려해야 한다.

#### 데이터 클렌징 작업과 유효성 검사

데이터 소스의 다양성만큼 소스 데이터의 품질도 다양하다.
데이터 파이프라인은 소스 데이터에 존재하는 한계와 결함을 이해하고 해결하는 역할을 수행해야 한다.

품질이 낮은 데이터는 몇 가지의 공통적인 특징이 있다.

1.   중복되거나 모호한 레코드
2.   고립된 레코드
3.   불완전하거나 누락된 레코드
4.   텍스트 인코딩 오류
5.   일치하지 않는 형식
6.   레이블이 잘못되었거나 레이블이 지정되지 않은 데이터

데이터 엔지니어는 이런 데이터들의 품질을 개선하기 위해 몇가지 원칙을 가지고 파이프라인을 설계해야 한다.

##### 최악을 가정하고 최상을 기대하라

-   입력 데이터 셋에 수많은 유효성과 일관성 문제가 있음에도 불구하고 깨끗한 출력을 위해 데이터를 식별하고 정리하는 파이프라인을 구축한다고 가정한다.

##### 가장 적합한 시스템에서 데이터를 정리하고 검증하라

-   파이프라인에서 나중에 데이터를 정리할 때 까지 좋은 방식일 때가 있다.
-   데이터 엔지니어는 데이터 클렌징과 검증 프로세스를 서두르지 말고 올바른 작업에 올바른 도구를 사용해야 한다.

##### 자주 검증하라

-   파이프라인 데이터를 여러 포인트에서 자주 검증한다.

#### 소스 시스템의 지연 시간 및 대역폭

최신 데이터 스택에서는 소스 시스템에서 대량의 데이터를 자주 추출하는 것이 일반적이다.

API 속도 제한, 연결 시간 초과, 느린 다운로드, 시스템에 대한 부담등을 고려해 데이터 파이프라인을 구축해야 대량의 데이터를 자주 추출할 수 있다.

#### 클라우드 데이터 웨어하우스 및 데이터 레이크

퍼블릭 클라우드 공급업체가 등장하며 데이터 웨어하우징과 분석 환경에도 변화가 있었다.

1.   클라우드에서 데이터 파이프라인, 데이터 레이크, 데이터 웨어하우스의 구축 및 배포가 쉬워졌고 클라우드 공급업체에서 관리해주는 관리 서비스(DB 등)가 주류가 되었다.

2.   지속적인 클라우드 내 스토리지 비용이 감소했다..

3.   Amazon Redshift, Snowflake, Google Big Query 등 확장성 뛰어난 열 기반 DB가 등장했다.

위와 같은 변화는 데이터 레이크라는 개념이 도입되는데 큰 역할을 했다.

데이터 레이크란 데이터가 저장되나 데이터 웨어하우스처럼 데이터 구조 통일이나 쿼리 최적화 필요 없이 다양한 부서의 다양한 유형을 가진 데이터들을 집대성한 곳이다.

단일 데이터 레이크에는 텍스트, 이미지 등 여러 유형과 구조를 가진 데이터들이 포함되어 있다.

데이터 파이프라인은 데이터 웨어하우스와 데이터 레이크 간 빈번한 데이터 이동을 담당하게 된다.

#### 데이터 수집 도구

이 책에서는 일반적인 데이터 수집 도구와 프레임 워크에 대해 설명한다.

1.   Singer
2.   Stitch
3.   Fivetran

#### 데이터 변환 및 모델링 도구

데이터 변환과 데이터 모델링은 종종 같은 의미로 사용되기도 하나 정확한 학습을 위해 두 용어를 구분해 설명한다.

-   데이터 변환
    -   ETL이나 ELT 프로세스에서 T에 해당하는 광범위한 용어로, 저장된 타임스탬프를 한 시간대에서 다른 시간대로 변환한 것처럼 간단할 수 도 비즈니스 로직을 통해서 집계되고 필터링 된 여러 원본 열을 바탕으로 새 지표를 생성하는 것 처럼 복잡할 수도 있는 작업을 총칭한다.
-   데이터 모델링
    -   보다 구체적인 데이터 변환의 한 유형으로, 데이터 분석을 위해 데이터를 이해하고 최적화된 형식으로 정형화하고 정의하는 과정이다.

#### 워크플로 오케스트레이션 플랫폼

데이터 파이프라인이 복잡해지고 많아지며 데이터 인프라에 워크플로 오케스트레이션 플랫폼을 도입하는 것이 중요한 과제가 되었다.

Apache Airflow나 Luigi, AWS Glue와 같은 범용적인 오케스트레이션 플랫폼도 있고, Kubeflow Pipelin과 같이 ML에 특화된 오케스트레이션 플랫폼도 존재한다.

#### 방향성 비순환 그래프

거의 모든 최신 오케스트레이션 프레임워크는 각 작업의 흐름과 종속성을 그래프로 나타낸다.
이런 그래프들은 몇 가지 특정 제약 조건이 있다.

1.   파이프라인 단계는 하나 또는 여러 개의 작업으로부터 다른 작업을 향하는 방향성을 가진다.
2.   파이프라인 그래프는 비순환 그래프여야 한다.

#### 데이터 인프라 커스터마이징

데이터 인프라는 조직의 목적과 특성에 따라 다르게 설계된다.
대부분은 특정 요구사항을 충족하는 도구와 공급 업체를 선택하고 나머지를 자체적으로 구축한다.

각 조직의 제약 조건(비용, 엔지니어링 리소스, 보안 및 법적 리스크 허용 범위)과 그에 해당하는 트레이드 오프를 이해하고 데이터 인프라에 대한 의사 결정을 내려야 한다.