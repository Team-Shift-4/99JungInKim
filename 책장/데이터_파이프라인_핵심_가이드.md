# 데이터 파이프라인 핵심 가이드

## 데이터 파이프라인 소개

### 데이터 파이프라인이란?

다양한 소스에서 새로운 가치를 얻을 수 있는 대상으로 데이터를 옮기고 변환하는 일련의 과정이다.
분석, 리포팅, 머신러닝 능력의 기초가 된다.

데이터 파이프라인은 Rest API와 같은 단일 소스에서 데이터를 추출하고 데이터 웨어하우스의 SQL 테이블과 같은 다른 대상으로 데이터를 옮기는 간단한 구조의 파이프라인부터 추출된 데이터에 대한 유효성 검사, 가공 과정, 머신러닝 모델 추론과 같은 과정들이 추가되는 복잡한 구조의 파이프라인까지 다양한 복잡성을 가진다.

### 누가 파이프라인을 구축할까?

데이터 파이프라인은 누구라도 구축 가능하나 주로 데이터 엔지니어가 구축한다.

데이터 엔지니어의 목적은 단순히 데이터를 데이터 웨어하우스에 로드하는 것 뿐만 아니라 데이터를 통해 통찰력을 만들어내는 데이터 과학자, 데이터 분석가와 협업해 분석에 집중할 수 있도록 확장 가능한 프로덕션 상태로 전환하는 것이다.

데이터 엔지니어의 특정 기술은 조직에서 사용하는 기술 스택에 따라 달라지나 몇몇 공통적인 보유 기술이 있다.

1.   SQL과 데이터 웨어하우징 기초
     -   DB를 다루기 위해 쿼리하는 방법을 알아야 한다.
         SQL은 DB 쿼리에 보편적인 언어이다.
     -   단순 SQL 뿐만 아니라 SQL 튜닝법(고성능 SQL 작성법)과 데이터 모델링의 기본 사항을 알아야 한다.
2.   Python, Java
     -   주로 Python, Java를 사용한다.
     -   최근에는 Go 언어도 등장한다.
3.   분산 컴퓨팅
     -   데이터 양이 많아지며 분산 컴퓨팅 플랫폼이 사용되고 있다.
         분산 컴퓨팅은 여러 시스템의 성능을 결합해 대량의 데이터를 효율적으로 저장, 처리, 분석할 수 있다.
4.   기본 시스템 관리
     -   데이터 엔지니어는 리눅스 명령줄에 능숙해야 한다.
     -   응용 프로그램 로그 분석, 크론 작업 예약, 방화벽 및 기타 보안 설정과 같은 작업이 요구된다.
5.   목표 지향적 사고 방식
     -   데이터 엔지니어는 팀의 데이터 분석가, 데이터 과학자와 정기적으로 소통하고 협엽한다.
         파이프라인을 구축하는 이유를 알고 있으면 더 나은 아키텍처적인 결정을 내릴 수 있다.

### 왜 데이터 파이프라인을 구축할까?

데이터 과학자와 분석가는 데이터를 분석해 비즈니스적인 통찰력을 제공하는데 전문성이 있다.
원본 데이터를 수집하고 처리하는 데 시간을 낭비하지 않고 잘 정제된 데이터를 통해 분석 업무만을 수행해야 효율적이다.

데이터 과학자와 분석가가 데이터를 더 가치있고 효율적으로 사용할 수 있도록 하기 위해 데이터 파이프라인 구축이 필요하다.

## 최신 데이터 인프라

### 데이터 소스의 다양성

대부분의 조직에는 분석 작업의 수행 대상이 되는 수십 ~ 수백개의 데이터 소스가 존재한다.

데이터 소스들은 여러 차원으로 구분될 수 있다.

#### 소스 시스템 소유권

데이터 수집(Data Ingestion)이란 한 소스에서 데이터를 추출해 다른 소스로 옮기는 행위이다.

데이터 수집의 대상이 되는 소스 시스템은 위치에 따라 **타사 데이터 소스에 위치한 데이터**나 내부적으로 **구축된 시스템**으로 구별된다.

1.   타사 데이터 소스에 위치한 데이터 소스
     -   예시로 Rest API를 통한 데이터 접근은 제공하나 SQL DB에 직접 접근하지 못하게 하는 경우가 흔하다.
     -   통신 속도와 같은 제한이 생길 수 있고, 접근 가능한 데이터와 세부 수준까지 분석가가 필요로 하는 환경에 맞게 지정하기 어렵다.
2.   내부적으로 구축된 시스템
     -   접근 방법 뿐 아니라 데이터의 세부 수준을 분석가가 필요한 환경에 맞춰 지정할 수 있다.
     -   시스템을 구축하는 단계에서 발생할 수 있는 잠재적인 문제들을 고려해야 한다.
         -   데이터 수집이 시스템에 부하를 가하는가?
         -   데이터를 점진적으로 옮길 수 있는가?

#### 수집 인터페이스와 데이터 구조

소스 데이터의 소유권과 관계 없이 데이터 엔지니어는 소스 데이터를 얻는 방법과 형식을 우선 고려해야 한다.

##### 데이터 인터페이스

1.   PostgreSQL이나 MySQL DB와 같이 애플리케이션 뒤에 있는 DB
2.   Rest API와 같은 시스템 상단의 추상화 계층
3.   Apache Kafka와 같은 스트림 처리 플랫폼
4.   CSV 파일과 같이 기타 플랫 파일을 포함하는 공유 네트워크 파일 시스템(Network File System)나 클라우드 스토리지 버킷
5.   데이터 웨어하우스나 데이터 레이크
6.   HDFS이나 HBase 데이터베이스의 데이터

##### 데이터 구조

1.   Rest API의 JSON
2.   DB의 잘 구성된 데이터
3.   DB 테이블의 열 내의 JSON
4.   반정형화된 로그 데이터
5.   CSV, 고정 폭 형식(FWF)과 기타 플랫 파일 형식
6.   플랫 파일의 JSON
7.   Kafka의 스트림 출력

잘 구성된 데이터는 파이프라인 구축과 분석 작업을 진행하기 쉬울 수 있다.
하지만 해당 데이터는 일반적으로 애플리케이션이나 웹 사이트를 위해 정형화되어 있다.
분석 작업에 더 적합한 형태로 정형화하기 위해 데이터 수집 외에 클렌징, 변환 작업 등을 파이프라인에 구축해야 한다.

JSON과 같은 반정형 데이터의 경우 관계형 DB와 달리 같은 데이터 셋 안에 데이터의 무결성을 보장하기 어렵다.
같은 데이터 셋 내에 있더라도 누락되거나 구조가 다른 데이터가 포함될 수 있다(파이프라인에서 처리해줘야 한다.).

비정형 데이터는 자연어 처리(NLP)나 컴퓨터 비전(CV)과 같이 이미지, 비디오, 오디오, 텍스트 등의 데이터를 의미한다.
파이프라인 구성 시에 데이터 형식을 고려해줘야 한다.

#### 데이터 사이즈

데이터 엔지니어는 파이프라인의 각 단계를 설정할 때 데이터 사이즈를 고려한다.
데이터 사이즈가 크다고 가치가 비례하게 증가하지는 않기에 대용량 / 소용량 두 가지로 데이터 셋을 구별하기 보단 스펙트럼의 측면에서 데이터 사이즈를 고려해야 한다.

#### 데이터 클렌징 작업과 유효성 검사

데이터 소스의 다양성만큼 소스 데이터의 품질도 다양하다.
데이터 파이프라인은 소스 데이터에 존재하는 한계와 결함을 이해하고 해결하는 역할을 수행해야 한다.

품질이 낮은 데이터는 몇 가지의 공통적인 특징이 있다.

1.   중복되거나 모호한 레코드
2.   고립된 레코드
3.   불완전하거나 누락된 레코드
4.   텍스트 인코딩 오류
5.   일치하지 않는 형식
6.   레이블이 잘못되었거나 레이블이 지정되지 않은 데이터

데이터 엔지니어는 이런 데이터들의 품질을 개선하기 위해 몇가지 원칙을 가지고 파이프라인을 설계해야 한다.

##### 최악을 가정하고 최상을 기대하라

-   입력 데이터 셋에 수많은 유효성과 일관성 문제가 있음에도 불구하고 깨끗한 출력을 위해 데이터를 식별하고 정리하는 파이프라인을 구축한다고 가정한다.

##### 가장 적합한 시스템에서 데이터를 정리하고 검증하라

-   파이프라인에서 나중에 데이터를 정리할 때 까지 좋은 방식일 때가 있다.
-   데이터 엔지니어는 데이터 클렌징과 검증 프로세스를 서두르지 말고 올바른 작업에 올바른 도구를 사용해야 한다.

##### 자주 검증하라

-   파이프라인 데이터를 여러 포인트에서 자주 검증한다.

#### 소스 시스템의 지연 시간 및 대역폭

최신 데이터 스택에서는 소스 시스템에서 대량의 데이터를 자주 추출하는 것이 일반적이다.

API 속도 제한, 연결 시간 초과, 느린 다운로드, 시스템에 대한 부담등을 고려해 데이터 파이프라인을 구축해야 대량의 데이터를 자주 추출할 수 있다.

#### 클라우드 데이터 웨어하우스 및 데이터 레이크

퍼블릭 클라우드 공급업체가 등장하며 데이터 웨어하우징과 분석 환경에도 변화가 있었다.

1.   클라우드에서 데이터 파이프라인, 데이터 레이크, 데이터 웨어하우스의 구축 및 배포가 쉬워졌고 클라우드 공급업체에서 관리해주는 관리 서비스(DB 등)가 주류가 되었다.

2.   지속적인 클라우드 내 스토리지 비용이 감소했다..

3.   Amazon Redshift, Snowflake, Google Big Query 등 확장성 뛰어난 열 기반 DB가 등장했다.

위와 같은 변화는 데이터 레이크라는 개념이 도입되는데 큰 역할을 했다.

데이터 레이크란 데이터가 저장되나 데이터 웨어하우스처럼 데이터 구조 통일이나 쿼리 최적화 필요 없이 다양한 부서의 다양한 유형을 가진 데이터들을 집대성한 곳이다.

단일 데이터 레이크에는 텍스트, 이미지 등 여러 유형과 구조를 가진 데이터들이 포함되어 있다.

데이터 파이프라인은 데이터 웨어하우스와 데이터 레이크 간 빈번한 데이터 이동을 담당하게 된다.

#### 데이터 수집 도구

이 책에서는 일반적인 데이터 수집 도구와 프레임 워크에 대해 설명한다.

1.   Singer
2.   Stitch
3.   Fivetran

#### 데이터 변환 및 모델링 도구

데이터 변환과 데이터 모델링은 종종 같은 의미로 사용되기도 하나 정확한 학습을 위해 두 용어를 구분해 설명한다.

-   데이터 변환
    -   ETL이나 ELT 프로세스에서 T에 해당하는 광범위한 용어로, 저장된 타임스탬프를 한 시간대에서 다른 시간대로 변환한 것처럼 간단할 수 도 비즈니스 로직을 통해서 집계되고 필터링 된 여러 원본 열을 바탕으로 새 지표를 생성하는 것 처럼 복잡할 수도 있는 작업을 총칭한다.
-   데이터 모델링
    -   보다 구체적인 데이터 변환의 한 유형으로, 데이터 분석을 위해 데이터를 이해하고 최적화된 형식으로 정형화하고 정의하는 과정이다.

#### 워크플로 오케스트레이션 플랫폼

데이터 파이프라인이 복잡해지고 많아지며 데이터 인프라에 워크플로 오케스트레이션 플랫폼을 도입하는 것이 중요한 과제가 되었다.

Apache Airflow나 Luigi, AWS Glue와 같은 범용적인 오케스트레이션 플랫폼도 있고, Kubeflow Pipelin과 같이 ML에 특화된 오케스트레이션 플랫폼도 존재한다.

#### 방향성 비순환 그래프

거의 모든 최신 오케스트레이션 프레임워크는 각 작업의 흐름과 종속성을 그래프로 나타낸다.
이런 그래프들은 몇 가지 특정 제약 조건이 있다.

1.   파이프라인 단계는 하나 또는 여러 개의 작업으로부터 다른 작업을 향하는 방향성을 가진다.
2.   파이프라인 그래프는 비순환 그래프여야 한다.

#### 데이터 인프라 커스터마이징

데이터 인프라는 조직의 목적과 특성에 따라 다르게 설계된다.
대부분은 특정 요구사항을 충족하는 도구와 공급 업체를 선택하고 나머지를 자체적으로 구축한다.

각 조직의 제약 조건(비용, 엔지니어링 리소스, 보안 및 법적 리스크 허용 범위)과 그에 해당하는 트레이드 오프를 이해하고 데이터 인프라에 대한 의사 결정을 내려야 한다.

## 일반적인 데이터 파이프라인 패턴

### ETL과 ELT

데이터 웨어하우스와 비즈니스 인텔리전스에서 널리 사용되는 잘 알려진 패턴이다.
데이터 과학과 머신러닝 모델을 위한 파이프라인 패턴에 영감을 줬다.

ETL과 ELT는 데이터 웨어하우징에 근간을 둔다.
두 패턴 모두 데이터 웨어하우스에 데이터를 공급하고 분석가나 보고 도구가 이를 유용하게 사용할 수 있게 하는 데이터 처리에 대한 접근 방식이다.

ETL과 ELT의 단계는 Extract, Load, Transform으로 나뉜다.

| 한글         | 영어      | 설명                                                         |
| ------------ | --------- | ------------------------------------------------------------ |
| 추출         | Extract   | 로드 및 변환을 준비하기 위해 다양한 소스에서 데이터를 수집한다. |
| 로드(옮기기) | Load      | ELT의 경우 원본 데이터를, ETL의 경우 변환된 데이터를 최종 대상으로 가져온다.<br />최종 결과는 데이터 웨어하우스, 데이터 레이크 또는 기타 대상에 데이터를 로드하는 것이다. |
| 변환         | Transform | 분석가, 시각화 도구, 파이프라인이 제공하는 모든 사용 사례에 유용하게 쓸 수 있게 각 소스 시스템의 원본 데이터를 결합하고 형식을 지정하는 단계이다. |

### ETL을 넘어선 ELT의 등장

ETL은 수십 년 동안 데이터 파이프라인 패턴의 표준이였다.
최근에는 ELT라는 추가적인 선택지가 등장했다.

최신 유형의 데이터 웨어하우스 이전엔 데이터 팀이 방대한 양의 원본 데이터를 로드해 이를 사용 가능 데이터로 변환하는 데 필요한 스토리지나 컴퓨팅 자원이 모두 모여있는 데이터 웨어하우스에 접근할 수 없었다.

당시 데이터 웨어하우스는 트랜잭션 사용 사례에서 잘 작동하는 행 기반 DB였으므로 분석에서 흔히 볼 수 있는 대용량 쿼리에 적합하지 않았다.
따라서 데이터는 먼저 소스 시스템에서 추출된 후 웨어하우스에 로드되어 분석가와 시각화 도구에 의한 최종 데이터 모딜링과 쿼리를 하기 전 별도의 시스템에서 변환되었다.

오늘날 대부분의 데이터 웨어하우스는 비용 효율적인 방식으로 대규모 데이터 셋에 대한 대량 변환을 저장하고 실행할 수 있는 열 기반 DB를 기반으로 한다.
열 기반 DB들은 I/O 효율성, 데이터 압축, 데이터 처리를 위한 여러 병렬 노드에 데이터와 쿼리를 분산하는 기능이 있다.
이 덕분에 데이터를 추출하고 파이프라인을 완료하는 데 필요한 변환을 수행할 수 있는 데이터 웨어하우스에 로드하는 것에 집중할 수 있게 되었다.

| OrderId | CustomerId | ShippingContry | OrderTotal |
| ------- | ---------- | -------------- | ---------- |
| 1       | 1258       | US             | 55.25      |
| 2       | 5698       | AUS            | 125.36     |
| 3       | 2265       | US             | 776.95     |
| 4       | 8954       | CA             | 32.16      |

| Block   | Record Data          |
| ------- | -------------------- |
| Block 1 | 1, 1258, US, 55.25   |
| Block 2 | 2, 5698, AUS, 125.36 |
| Block 3 | 3,2265, US, 776.95   |
| Block 4 | 4, 8954, CA, 32.16   |

위 두 표는 행 기반 저장소 DB에 저장된 테이블의 예시이다.
각 블록에는 테이블의 레코드가 포함된다.

행 기반 DB의 경우에는 단일 행의 I/O에 최적화되어 있다.
하지만 빈 값의 경우 그 블럭의 해당 위치를 비워놓아야 해서 대량 변환과 저장에 있어 비교적 비효율적이다.

| OrderId | CustomerId | ShippingContry | OrderTotal | CustomerActive |
| ------- | ---------- | -------------- | ---------- | -------------- |
| 1       | 1258       | US             | 55.25      | TRUE           |
| 2       | 5698       | AUS            | 125.36     | TRUE           |
| 3       | 2265       | US             | 776.95     | TRUE           |
| 4       | 8953       | CA             | 32.16      | FALSE          |

| Block   | Column Data                  |
| ------- | ---------------------------- |
| Block 1 | 1, 2, 3, 4                   |
| Block 2 | 1258, 5698, 2265, 8954       |
| Block 3 | US, AUS, US, CA              |
| Block 4 | 55.25, 125.36, 776.95, 32.16 |
| Block 5 | TRUE, TRUE, TRUE, FALSE      |

위 두 표는 열 기반 저장소 DB에 저장된 테이블의 예시이다.
각 블록에는 테이블의 열이 포함된다.

열 기반 DB의 경우 필터링과 합산을 수행하기 위한 로드할 데이터와 디스크 I/O가 줄어든다.
따라서 저장소를 최적화 할 수 있다.
그 이유는 행 기반과 같이 여러 데이터 유형을 로드하는 것이 아니라 동일한 데이터 유형이 저장되므로 블록을 남김없이 활용하고 최적으로 압축할 수 있기 때문이다.

### EtLT 하위 패턴

ELT가 지배적인 패턴으로 등작했으 ㄹ때 추출 후 로드하기 전에 간단히 변환하는 것이 여전히 유익하다는 것이 분명해졌다.
하지만 비즈니스 논리나 데이터 모델링을 포함하는 변한 대신 이런 유형의 변환은 더 제한된다.
이것을 소문자 t 변환이나 EtLT라고 한다.

EtLT 하위 패턴에 맞는 변환 유형의 예

-   테이블에서 레코드 중복 제거
-   URL 파라미터를 개별 구성요소로 구문 분석
-   민감한 데이터 마스킹이나 난독화

이런 유형의 변환은 비즈니스 로직과 완전히 분리되거나 민감한 데이터를 마스킹하는 것과 같이 법적이나 보안상 이유로 파이프라인 초기에 필요한 경우가 있다.

### 데이터 분석을 위한 ELT

ELT는 데이터 분석 파이프라인에 있어 가장 일반적이고 최적의 패턴이 되었다.

ELT를 사용하면 데이터 엔지니어는 파이프라인의 추출 및 로드 단계에 집중할 수 있고 분석가는 SQL을 활용하여 보고 및 분석 용도로 수집된 데이터를 변환할 수 있다.
ETL 패턴에서 전체 파이프라인에 걸쳐 데이터 엔지니어가 필요하기에 이런 명확한 분리가 불가능하다.ELT를 사용하면 데이터 팀 구성원이 더 낮은 상호 의존성과 조정을 통해 ELT 자체의 강점에 집중 가능하다.

ETL 패턴은 추출 및 로드 프로세스를 구축할 때 분석가가 데이터로 수행할 작업을 정확히 예측해야 하는 필요성을 줄여준다.
적절한 데이터를 추출하고 로드하기 위해 일반적인 사용 사례를 이해해야 한다.
일반적인 사용단계를 이해해야 하나 변환 단계를 나중으로 넘김으로 분석가에게 더 많은 옵션과 유연성을 제공할 수 있다.

ETL의 등장으로 데이터 분석가는 데이터 엔지니어에 의해 차단되지 않고 데이터로부터 가치를 제공할 수 있는 자율성과 권한을 갖게 되었다.
데이터 엔지니어는 분석가가 SQL로 작성된 자체 변환 코드를 작성하고 배포할 수 있도록 지원하는 인프라와 데이터 수집에 집중할 수 있다.

### 데이터 과학을 위한 ELT

데이터 과학 팀을 위해 구축된 데이터 파이프라인은 데이터 웨어하우스에서 데이터 분석을 위해 구축된 파이프라인과 비슷하다.
분석 사용 사례와 마찬가지로 데이터 엔지니어는 데이터 웨어하우스나 데이터 레이크에 데이터를 수집하는 데 중점을 둔다.
데이터 과학자는 데이터 분석가와 다른 요구사항이 있다.

일반적으로 데이터 과학자는 데이터 분석가보다 더 세분화된 데이터에 접근해야 한다.
데이터 분석가가 지표를 생성하고 대시보드를 강화하는 데이터 모델을 구축하는 동안 데이터 과학자는 데이터를 탐색하고 예측 모델을 구축하는 데 하루를 보낸다.
데이터 과학자의 역할에 대한 세부 사항 중 상위수준의 구분은 데이터 과학자에게 서비스를 제공하는 파이프라인 설계에 중요하다.

데이터 과학자를 위한 파이프라인을 구축하는 경우 ELT 패턴의 추출과 로드 단계가 분석 지원과 거의 동일하다.
데이터 과학자는 ETL 파이프라인의 변환 단계에서 분석가를 위한 일부 데이터 모델을 사용해 이점을 얻을 수 있으나 추출 - 로드 중 획득한 많은 데이터를 분기해 사용할 확률이 높다.

### 데이터 제품 및 머신 러닝을 위한 ELT

데이터는 분석, 보고, 예측 모델 이상의 용도로 사용된다.
데이터 제품을 강력하게 만드는 데에도 사용된다.

데이터 제품의 예시

-   비디오 스트리밍 홈 화면을 구동하는 콘텐츠 추천 엔진
-   전자상거래 웹사이트의 개인화된 검색 엔진
-   사용자가 생성한 레스토랑 리뷰에 대한 감성 분석을 수행하는 애플리케이션

각 데이터 제품은 학습 및 검증 데이터를 필요로 하는 하나 이상의 ML(머신러닝) 모델에 의해 구동될 가능성이 높다.
이런 데이터는 다양한 소스 시스템에서 가져올 수 있으며 모델에서 사용할 수 있도록 일정 수준의 변환을 거칠 수 있다.
데이터 제품을 위해 설계된 파이프라인의 모든 단계에서 도전 과제가 있으나 ELT와 같은 패턴은 이런 요구사항에 부적합하다.

#### 머신 러닝 파이프라인의 단계

머신러닝용 파이프라인의 구축도 분석용과 마찬가지로 시작 부분에서는 ELT와 유사한 패턴을 가진다.
차이점으로 분석용에서는 변환 단계에서 데이터를 데이터 모델로 변환하는 데 중점을 두나 머신러닝용에서는 데이터가 추출되어 웨어하우스나 데이터 레이크에 로드되면 머신러닝 모델을 빌드하고 업데이트하는 것과 관련된 여러 단계가 있다.

##### 데이터 수집

수집하는 데이터는 다를 수 있으나 이 논리는 머신러닝뿐만 아니라 분석용으로 구축된 파이프라인에서도 동일하게 유지된다.
머신러닝용 파이프라인에서는 한 가지 추가 고려사항이 있다.
수집하는 데이터가 머신러닝 모델이 나중에 학습이나 검증을 위한 특정 데이터 셋으로 참조할 수 있도록 버전 지정이 되었는 지 확인해야 한다.

##### 데이터 전처리

수집된 데이터는 머신러닝 개발에 사용할 준비가 되지 않았다.
전처리는 데이터를 정리하고 모델에 사용할 준비를 하는 단계이다.

##### 모델 교육

새 데이터를 수집하고 전처리한 후 머신러닝 모델을 다시 학습해야 한다.

##### 모델 배포

모델을 운영 환경에 배포하는 것은 연구 중심의 머신러닝을 진정한 데이터 제품으로 전환하는 데 있어 가장 어려운 부분일 수 있다.
데이터 셋의 버전 관리 뿐 아니라 학습된 모델 버전 관리도 필요하다.
배포된 모델의 쿼리를 허용하는 데 Rest API가 사용되며 다양한 버전의 모델에 대한 API Endpoint가 사용된다.
운영 수준에 도달하기 위해 데이터 과학자, 머신러닝 엔지니어, 데이터 엔지니어 사이에서 추적하고 조정해야 할 일이 많다.
잘 설계된 파이프라인들은 위 사람들을 한데 묶는 데 있어 중요한 역할을 한다.

#### 파이프라인에 피드백 통합

좋은 머신러닝 파이프라인에는 모델 개선을 위한 피드백 수집도 포함된다.
이벤트 수집 예에는 그것을 추천한 모델의 버전이나 해당 항목을 클릭했을 때, 그리고 이후 사용자의 콘텐츠 소비와 관련해 이미 수집하고 있는 데이터로 클릭 연결을 수행하는 것이 포함된다.

그 이후 모든 정보를 데이터 웨어하우스로 다시 수집하고 학습 데이터나 미래 모델이나 실험에 쓰기 위해 사람(데이터 과학자)이 분석, 고려하기 위한 모델의 향후 버전에 통합할 수 있다.

수집된 데이터는 ELT 패턴으로 데이터 분석가가 수집, 변환, 분석할 수 있다.

분석가는 종종 모델의 효율성을 측정하고 조직에 모델의 주요 지표를 표시하기 위한 대시보드를 구축하는 임무를 맡게 된다.
이해 관계자는 이런 대시보드를 이용해 다양한 모델이 비즈니스와 고객에게 얼마나 효과적인지 알 수 있다.

#### ML 파이프라인에 대한 추가 자료

머신러닝 모델을 위한 파이프라인 구축은 강력한 주제다.
인프라 선택과 머신러닝 환경의 복잡성에 따라 추가 학습을 위해 추천하는 책 리스트이다

>   Building Machine Learning Pipelines
>
>   핸즈온 머신러닝
>
>   파이썬 라이브러리를 활용한 머신러닝

## 데이터 수집: 데이터 추출

